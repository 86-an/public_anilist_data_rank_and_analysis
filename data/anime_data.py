import requests
import time
import json

# ä¿å­˜ç”¨ãƒªã‚¹ãƒˆï¼ˆå–å¾—ã—ãŸã™ã¹ã¦ã®ã‚¢ãƒ‹ãƒ¡ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ï¼‰
all_anime_data = []

url = "https://graphql.anilist.co"

query = """
query ($page: Int) {
  Page(page: $page, perPage: 50) {
    pageInfo {
      hasNextPage
    }
    media(type: ANIME, sort: POPULARITY_DESC) {
      id
      title {
        romaji
        native
      }
      format
      season
      seasonYear
      favourites
      meanScore
      popularity
      genres
      source
      episodes
      description
      countryOfOrigin
      studios {
        edges {
          node {
            id
            name
            isAnimationStudio
          }
        }
      }
      characters(sort: FAVOURITES_DESC) {
        edges {
          node {
            id
            name {
              userPreferred
              native
            }
            favourites
          }
          voiceActors(language: JAPANESE, sort: FAVOURITES_DESC) {
            id
            name {
              userPreferred
              native
            }
            favourites
          }
        }
      }
      staff(sort: FAVOURITES_DESC) {
        edges {
          role
          node {
            id
            name {
              userPreferred
              native
            }
            favourites
          }
        }
      }
    }
  }
}
"""

def fetch_anime(page):
    """æŒ‡å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸ç•ªå·ã®ã‚¢ãƒ‹ãƒ¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ï¼ˆäººæ°—é †ï¼‰"""
    variables = {"page": page}
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
    }

    try:
        response = requests.post(url, json={"query": query, "variables": variables}, headers=headers)
        response.raise_for_status()
        data = response.json()

        if 'errors' in data:
            print(f"APIã‹ã‚‰ã®ã‚¨ãƒ©ãƒ¼: {data['errors']}")
            return None

        return data

    except requests.exceptions.RequestException as e:
        print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™å¯¾ç­–
request_count = 0

import re

# ãƒšãƒ¼ã‚¸ã‚’å›ã—ã¦å–å¾—
page = 1
is_last_page = False

while not is_last_page:
    data = fetch_anime(page)

    if data and 'data' in data and data['data']['Page']['media']:
        print(f"âœ… Page {page} å–å¾—å®Œäº†")

        cleaned_media = []
        for media in data['data']['Page']['media']:
            desc = media.get("description")
            if desc:
                # HTMLã‚¿ã‚°ã‚’é™¤å»
                media["description"] = re.sub(r'<[^>]+>', '', desc)
            cleaned_media.append(media)

        all_anime_data.extend(cleaned_media)

        if not data['data']['Page']['pageInfo']['hasNextPage']:
            is_last_page = True
        page += 1
    else:
        is_last_page = True
        print(f"âš ï¸ Page {page} ã§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    request_count += 1
    if request_count % 30 == 0:
        print("â³ 30ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ°é”ã€60ç§’ä¼‘æ­¢ä¸­...")
        time.sleep(60)
    else:
        time.sleep(2)

print("å…¨ã¦ã®äººæ°—é †ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# ğŸ”½ JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
with open("anilist_rank_data_analysis_popular_all_anime.json", "w", encoding="utf-8") as f:
    json.dump(all_anime_data, f, ensure_ascii=False, indent=2)

print("âœ… anilist_rank_data_analysis_popular_all_anime.json ã«ä¿å­˜å®Œäº†")